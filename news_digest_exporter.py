import feedparser
import datetime
import webbrowser
import os
import re
import html
import json
from urllib.parse import urlparse
from jinja2 import Template


_WS_RE = re.compile(r"\s+")

def clean_text(s: str) -> str:
    if not s:
        return ""
    # 1) &nbsp; ê°™ì€ HTML ì—”í‹°í‹°ë¥¼ ë¬¸ìë¡œ ë³€í™˜
    s = html.unescape(s)

    # 2) NBSP(ìœ ë‹ˆì½”ë“œ) -> ì¼ë°˜ ìŠ¤í˜ì´ìŠ¤ë¡œ
    s = s.replace("\u00a0", " ")

    # 3) í˜¹ì‹œ ì„ì—¬ ë“¤ì–´ì˜¨ HTML íƒœê·¸ ì œê±°
    s = re.sub(r"<[^>]+>", "", s)

    # 4) ê³µë°± ì •ë¦¬
    s = _WS_RE.sub(" ", s).strip()
    return s

# ==========================================
# ì‚¬ìš©ì ì„¤ì •
# ==========================================

RSS_SOURCES = [
    # ==========================
    # ë¡œë´‡ (KR + Global)
    # ==========================
    {
        "topic": "ë¡œë´‡",
        "url": "https://news.google.com/rss/search?q=ë¡œë´‡&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "ë¡œë´‡",
        "url": "https://news.google.com/rss/search?q=robotics+OR+robot&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # AGI / ê³ ê¸‰ AI (KR + Global)
    # ==========================
    {
        "topic": "AGI / ê³ ê¸‰ AI",
        "url": "https://news.google.com/rss/search?q=AGI&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "AGI / ê³ ê¸‰ AI",
        "url": "https://news.google.com/rss/search?q=AGI&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # AI / ì¸ê³µì§€ëŠ¥ (KR + Global)
    # ==========================
    {
        "topic": "AI / ì¸ê³µì§€ëŠ¥",
        "url": "https://news.google.com/rss/search?q=AI&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "AI / ì¸ê³µì§€ëŠ¥",
        "url": "https://news.google.com/rss/search?q=AI&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # ë°˜ë„ì²´ (KR + Global)
    # ==========================
    {
        "topic": "ë°˜ë„ì²´",
        "url": "https://news.google.com/rss/search?q=ë°˜ë„ì²´&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "ë°˜ë„ì²´",
        "url": "https://news.google.com/rss/search?q=semiconductor&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # íƒœì–‘ê´‘ / ì—ë„ˆì§€ ì „í™˜ (KR + Global)
    # ==========================
    {
        "topic": "íƒœì–‘ê´‘ / ì—ë„ˆì§€ ì „í™˜",
        "url": "https://news.google.com/rss/search?q=íƒœì–‘ê´‘&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "íƒœì–‘ê´‘ / ì—ë„ˆì§€ ì „í™˜",
        "url": "https://news.google.com/rss/search?q=solar+energy+OR+renewable+energy&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # ë°”ì´ì˜¤ / í—¬ìŠ¤ì¼€ì–´ (KR + Global)
    # ==========================
    {
        "topic": "ë°”ì´ì˜¤ / í—¬ìŠ¤ì¼€ì–´",
        "url": "https://news.google.com/rss/search?q=ë°”ì´ì˜¤+í—¬ìŠ¤ì¼€ì–´&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "ë°”ì´ì˜¤ / í—¬ìŠ¤ì¼€ì–´",
        "url": "https://news.google.com/rss/search?q=bio+healthcare+biotech&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },

    # ==========================
    # ê·œì œ / ë²•Â·ì •ì±… (í˜„ì¬ëŠ” í•œêµ­ ìœ„ì£¼)
    # ==========================
    {
        "topic": "ê·œì œ / ë²•Â·ì •ì±…",
        "url": "https://news.google.com/rss/search?q=ê·œì œ&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },

    # ==========================
    # ì²­ë…„ (í•œêµ­ ì´ìŠˆ ìœ„ì£¼)
    # ==========================
    {
        "topic": "ì„œìš¸",
        "url": "https://news.google.com/rss/search?q=ì„œìš¸&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },

        # ==========================
    # ì²­ë…„ (í•œêµ­ ì´ìŠˆ ìœ„ì£¼)
    # ==========================
    {
        "topic": "ê³ ìš©",
        "url": "https://news.google.com/rss/search?q=ê³ ìš©&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },

    # ==========================
    # ê¸ˆìœµ / ìë³¸ì‹œì¥ (KR + Global ì˜ˆì‹œ)

    # ==========================
    {
        "topic": "ê¸ˆìœµ / ìë³¸ì‹œì¥",
        "url": "https://news.google.com/rss/search?q=ê¸ˆìœµ+ìë³¸ì‹œì¥&hl=ko&gl=KR&ceid=KR:ko",
        "limit": 3,
    },
    {
        "topic": "ê¸ˆìœµ / ìë³¸ì‹œì¥",
        "url": "https://news.google.com/rss/search?q=finance+capital+market&hl=en&gl=US&ceid=US:en",
        "limit": 3,
    },
]

USE_AI_SUMMARY = False  # ë‚˜ì¤‘ì— Trueë¡œ ë°”ê¿”ì„œ í™œì„±í™”

def generate_ai_summary(title: str, summary: str, topic: str) -> str:
    """
    (ì„ íƒ) AI ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ìë¦¬.
    - ì§€ê¸ˆì€ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    - ë‚˜ì¤‘ì— OpenAI ë“± ë¶™ì¼ ë•Œ ì´ í•¨ìˆ˜ ì•ˆë§Œ êµ¬í˜„.
    """
    if not USE_AI_SUMMARY:
        return ""

    # ì˜ˆì‹œ (ë‚˜ì¤‘ì— ì‹¤ì œ API ë¶™ì¼ ë•Œ ì‚¬ìš©)
    """
    import openai
    prompt = f'''
    ì œëª©: {title}
    ì£¼ì œ: {topic}
    ë‚´ìš© ìš”ì•½: {summary}
    
    ìœ„ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ 2~3ì¤„ë¡œ í•µì‹¬ë§Œ ì§§ê²Œ ìš”ì•½í•´ì¤˜.
    '''
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()
    """
    raise NotImplementedError(
        "USE_AI_SUMMARY=True ì´ì§€ë§Œ AI ìš”ì•½ ê¸°ëŠ¥ì´ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    )

def generate_key_terms(title: str, summary: str, topic: str) -> list[str]:
    """
    ê¸°ì‚¬ ì œëª©/ìš”ì•½/í† í”½ì„ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ìë¦¬.
    - ì§€ê¸ˆì€ USE_AI_KEYWORDS=Falseë¼ì„œ í•­ìƒ [] ë°˜í™˜
    - ë‚˜ì¤‘ì— OpenAI ê°™ì€ LLM ë¶™ì¼ ë•Œ ì´ í•¨ìˆ˜ ì•ˆë§Œ êµ¬í˜„í•˜ë©´ ë¨.
    """
    if not USE_AI_KEYWORDS:
        return []

    # ì•„ë˜ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ êµ¬ì¡° ì˜ˆì‹œ (ì§€ê¸ˆì€ ì£¼ì„ ì²˜ë¦¬ìš©)
    """
    import openai

    prompt = f'''
    ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ê³µë¶€/ì‹œì¥ ë¶„ì„ì— ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 3~7ê°œ ë½‘ì•„ì¤˜.
    - ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´(ë‰´ìŠ¤, ì˜¤ëŠ˜, ë³´ë„, ê¸°ì ë“±)ëŠ” ì œì™¸.
    - ê¸°ìˆ , ì‚°ì—…, ê¸°ì—…, êµ­ê°€, ì •ì±…, ê·œì œ, ê°œë… ë‹¨ìœ„ ìœ„ì£¼ë¡œ.
    - í•œêµ­ì–´/ì˜ì–´ í˜¼ìš© ê°€ëŠ¥. ê° í‚¤ì›Œë“œëŠ” í•œë‘ ë‹¨ì–´ ê¸¸ì´ë¡œ.
    - ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œë§Œ ì¶œë ¥.

    [í† í”½] {topic}
    [ì œëª©] {title}
    [ìš”ì•½] {summary}
    '''

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    raw = resp.choices[0].message.content.strip()
    """

    raw = ""  # â†‘ ë‚˜ì¤‘ì— ì‹¤ì œ LLM ì‘ë‹µ ë¬¸ìì—´ ë„£ì„ ìë¦¬

    if not raw:
        return []

    # "AGI, ê·œì œ, ë¯¸êµ­, ë°˜ë„ì²´" â†’ ["AGI", "ê·œì œ", "ë¯¸êµ­", "ë°˜ë„ì²´"]
    terms = [t.strip() for t in raw.split(",") if t.strip()]
    return terms


NEWSLETTER_TITLE = "ğŸš€ DAILY WORLD â€“ AI & Tech ì¼ì¼ ìš”ì•½"

AFFILIATE_AD_TEXT = "ğŸ”¥ ì˜¤ëŠ˜ë§Œ 50% í• ì¸! ìµœê³ ì˜ ìƒì‚°ì„± ë„êµ¬ êµ¬ê²½í•˜ê¸°"
AFFILIATE_LINK = "https://your-affiliate-link.com"

OUTPUT_FILENAME = "daily_world_news.html"

# ===== MVP JSON ì¶œë ¥ ì„¤ì • =====
OUTPUT_JSON = "daily_digest.json"
# Lovable í™”ë©´ ìƒë‹¨ì— ë…¸ì¶œí•  ê³ ì • ë¬¸êµ¬ë“¤
SELECTION_CRITERIA = "â‘  ë‚´ì¼ë„ ì˜í–¥ì´ ë‚¨ëŠ” ì´ìŠˆ â‘¡ ê³¼ë„í•œ ê°ì • ì†Œëª¨ ì œì™¸ â‘¢ ì–´ì œì™€ ì¤‘ë³µë˜ëŠ” ë‰´ìŠ¤ ì œì™¸"
EDITOR_NOTE = "ì´ ë‰´ìŠ¤ëŠ” í´ë¦­ ìˆ˜ê°€ ì•„ë‹ˆë¼ ì˜¤ëŠ˜ ì´í›„ì—ë„ ë‚¨ëŠ” ì •ë³´ë§Œ ê¸°ì¤€ìœ¼ë¡œ í¸ì§‘í–ˆìŠµë‹ˆë‹¤."
QUESTION_OF_THE_DAY = "ì •ë³´ë¥¼ ëœ ë³´ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ë” ë˜‘ë˜‘í•œ ì†Œë¹„ì¼ê¹Œ?"

# í‚¤ì›Œë“œ ìë™ ìƒì„±(LLM) ê¸°ëŠ¥ í† ê¸€ (í˜„ì¬ëŠ” ë¹„í™œì„± ê¶Œì¥)
USE_AI_KEYWORDS = False


TOP_LIMIT = 5  # ì „ì²´ TOP N (MVP: 5ê°œ ê³ ì •)
# topicë³„ ìµœëŒ€ ê¸°ì‚¬ ê°œìˆ˜ëŠ” ê° topicì— ì„¤ì •ëœ limit ì¤‘ ìµœëŒ€ê°’ì„ ì‚¬ìš© (ë’¤ì—ì„œ ê³„ì‚°)


# ==========================================
# íë ˆì´ì…˜ ê¸°ì¤€ (ì—¬ê¸° ìœ„ì£¼ë¡œ íŠœë‹)
# ==========================================



QUALITY_KEYWORDS = [
    "ë¶„ì„", "í•´ì„¤", "ì „ë§", "ì‹¬ì¸µ", "ì§„ë‹¨",
    "ì „ëµ", "íŒ¨ê¶Œ", "íŒ¨ëŸ¬ë‹¤ì„", "ë³€ê³¡ì ", "êµ¬ì¡°", "ì¬í¸", "ì§€í˜•",
    "ëª¨ë©˜í…€", "êµ¬ì¡°ì ", "ìƒíƒœê³„", "ì‹œë‚˜ë¦¬ì˜¤",
    "data", "in-depth", "diagnosis", "strategy", "paradigm",
    "inflection point", "structure", "reorganization", "ecosystem", "scenario",
]


HARD_EXCLUDE_KEYWORDS = [
    # ë¦¬í¬íŠ¸/ê¸°ê´€/í™ë³´/í–‰ì‚¬/ëª¨ì§‘
    "ë™í–¥", "ë™í–¥ë¦¬í¬íŠ¸", "ë¦¬í¬íŠ¸", "ë¸Œë¦¬í”„", "ë°±ì„œ", "ìë£Œì§‘", "ë³´ê³ ì„œ", "ì—°êµ¬ë³´ê³ ì„œ",
    "ì„¸ë¯¸ë‚˜", "ì›¨ë¹„ë‚˜", "ì»¨í¼ëŸ°ìŠ¤", "í¬ëŸ¼", "í–‰ì‚¬", "ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜",
    "ë³´ë„ìë£Œ", "í™ë³´", "í”„ë¡œëª¨ì…˜", "í• ì¸", "ì¶œì‹œê¸°ë…",
    # ì˜ë¬¸
    "whitepaper", "report", "brief", "webinar", "conference", "forum",
    "press release", "promotion", "apply now",
]

HARD_EXCLUDE_URL_HINTS = [
    "/report", "/whitepaper", "/webinar", "/seminar", "/conference", "/event", "/download"
]


EXCLUDE_KEYWORDS = [
    # ì—°ì˜ˆ/ê°€ì‹­
    "ì—°ì˜ˆ", "ìŠ¤íƒ€", "ê±¸ê·¸ë£¹", "ë³´ì´ê·¸ë£¹", "ì•„ì´ëŒ",
    "ë°°ìš°", "ê°€ìˆ˜", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì˜í™”", "íŒ¬ë¯¸íŒ…",
    "ì»´ë°±", "ì•¨ë²”", "ë®¤ì§ë¹„ë””ì˜¤", "ë®¤ë¹„", "í‹°ì €", "í™”ë³´",
    "ì—´ì• ", "ê²°ë³„", "ì´í˜¼", "ê²°í˜¼", "ì¶œì‚°",

    # ìŠ¤í¬ì¸ 
    "ì•¼êµ¬", "ì¶•êµ¬", "ë†êµ¬", "ë°°êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ",
    "Kë¦¬ê·¸", "KBO", "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸", "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸",

    # ì‚¬ê±´ì‚¬ê³ (ì¹˜ëª…ì ì¸ ë²”ì£„/ìê·¹ì  ë³´ë„)
    "ì‚´í•´", "ì‚´ì¸", "í­í–‰", "ì„±í­í–‰", "ê°•ê°„", "ë‚©ì¹˜",
    "ì‚¬ë§", "ì‹œì‹ ",  "ì§•ì—­", 

    # ë„ˆë¬´ ë¡œì»¬í•œ ìƒí™œ/ê°€ì‹­
    "ë§›ì§‘", "ì¹´í˜", "ë·°ë§›ì§‘", "ì—¬í–‰ê¸°", "ê´€ê´‘ì§€", "ì—°íœ´",
    "ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "êµí†µí†µì œ",

    # ê·¸ ì™¸ (ìê·¹/í´ë¦­ë² ì´íŠ¸)
    "ê²½ì•…", "ë°œì¹µ", "ì•Œê³ ë³´ë‹ˆ", "ì´ìœ ëŠ”", "ê·¼í™©",
    "í¬ì°©", "ë§ì‹ ", "ëˆ„ë¦¬ê¾¼", "ê°‘ë¡ ì„ë°•", "ê²°êµ­", "ì •ì²´", "ì¶©ê²©", "í—‰", "ì†Œë¦„", "ì´ê²Œ ì–¼ë§ˆ", "ëŒ€ì°¸ì‚¬", "ëŒ€ë°•",
    "ì£¼ì˜ë³´", "ë ˆì „ë“œ", "ì›ƒìŒ", "ì›ƒê²¼", "ëˆˆë¬¼",

    # Entertainment / Gossip (ì˜ë¬¸)
    "entertainment", "celebrity", "girl group", "boy group", "idol",
    "actor", "singer", "variety show", "drama", "movie", "fan meeting",
    "comeback", "album", "music video", "teaser", "photoshoot",
    "dating", "breakup", "divorce", "marriage", "childbirth",

    # Sports (ì˜ë¬¸)
    "baseball", "soccer", "basketball", "volleyball", "golf", "esports",
    "K League", "KBO", "Premier League", "Champions League",

    # Crime / Sensational Incidents (ì˜ë¬¸)
    "murder", "killing", "assault", "sexual assault", "rape", "kidnapping",
    "death", "corpse", "police", "arrest", "detention",
    "trial", "prison sentence", "lawsuit",

    # Local lifestyle/gossip (ì˜ë¬¸)
    "restaurant", "cafe", "tour spot", "travel diary", "tourism", "holiday",
    "weather", "fine dust", "traffic control",

    # Sensational / Clickbait (ì˜ë¬¸)
    "shock", "scandal", "caught on camera", "backlash", "controversy",
    "reason why", "latest update", "netizens", "argument", "eventually",
    "identity", "disaster", "huge", "warning", "legendary",
    "funny", "laughter", "tearful",

     # ì§€ì—­/ìƒí™œ/í–‰ì‚¬/ê³µëª¨ ë“± 
   "ìì‚¬ë¬´ì†Œ", "ë©´ì‚¬ë¬´ì†Œ", "ë§ˆì„íšŒê´€", "ì²´í—˜ í–‰ì‚¬", "ì§€ì—­ ì†Œì‹",
   "ì „í†µì‹œì¥", "ì§€ì—­ì£¼ë¯¼", "ë§ˆì„ ì£¼ë¯¼",
    "ë†ì´Œ ì²´í—˜", "ì–´ì´Œ ì²´í—˜", "ì§€ì—­ ì¶•ì œ", "êµ°ë¯¼",
    "ê³µëª¨ ì‚¬ì—…", 
]

SOURCE_TIER_A = {"Reuters", "Bloomberg", "Financial Times", "The Wall Street Journal", "ì—°í•©ë‰´ìŠ¤", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ", "ì„œìš¸ê²½ì œ"}
SOURCE_TIER_B = {"ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì „ìì‹ ë¬¸", "ZDNet Korea", "TechCrunch", "The Verge"}


MIN_SCORE = 2.0
MAX_ENTRIES_PER_FEED = 100

# HTML íƒœê·¸ ì œê±°ìš© ì •ê·œì‹
TAG_RE = re.compile(r"<[^>]+>")


# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================
def pick_top_with_mix(all_items, top_limit=5):
    buckets = {"IT": [], "ê²½ì œ": [], "ê¸€ë¡œë²Œ": []}
    for it in all_items:
        cat = map_topic_to_category(it.get("topic", ""))
        buckets[cat].append(it)

    for cat in buckets:
        buckets[cat].sort(key=lambda x: x["score"], reverse=True)

    target = {"IT": 2, "ê²½ì œ": 2, "ê¸€ë¡œë²Œ": 1}
    picked = []
    for cat, n in target.items():
        picked += buckets[cat][:n]

    # ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ì¶”ê°€
    if len(picked) < top_limit:
        remain = [x for x in sorted(all_items, key=lambda x: x["score"], reverse=True) if x not in picked]
        picked += remain[: top_limit - len(picked)]

    return picked[:top_limit]


def source_weight(source_name: str) -> float:
    if source_name in SOURCE_TIER_A:
        return 3.0
    if source_name in SOURCE_TIER_B:
        return 1.5
    return 0.3


def trim_title_noise(title: str) -> str:
    # ë„ˆë¬´ ê³µê²©ì ì´ë©´ ìœ„í—˜í•˜ë‹ˆ, ìš°ì„  ' | ' í•œ ë²ˆë§Œ ì»·
    return title.split(" | ")[0].strip()

def get_source_name(entry) -> str:
    """Google News RSSì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„(source.title)ì„ ê°€ì ¸ì˜´."""
    try:
        if hasattr(entry, "source") and hasattr(entry.source, "title"):
            return entry.source.title.strip()
    except Exception:
        pass
    return ""


def score_entry(entry) -> float:
    """
    RSS entry í•˜ë‚˜ì— ëŒ€í•´ 'ì–‘ì§ˆ + êµ¬ì¡°ì  ì¤‘ìš”ë„' ì ìˆ˜ ê³„ì‚°.
    - ì–¸ë¡ ì‚¬ ì´ë¦„
    - ì¸ì‚¬ì´íŠ¸ í‚¤ì›Œë“œ
    - ì—°ì˜ˆ/ê°€ì‹­/ì‚¬ê±´ì‚¬ê³  í•„í„°ë§ (í•˜ë“œ í•„í„°)
    - ìµœì‹ ì„±
    - ìš”ì•½ ê¸¸ì´
    """
    score = 0.0

    


    title_raw = getattr(entry, "title", "") or ""
    summary_raw = getattr(entry, "summary", "") or ""

    if "|" in title_raw or ">" in title_raw or "â€¦" in title_raw or "..." in title_raw:
        score -= 1.0

    title = trim_title_noise(clean_text(title_raw))
    summary = clean_text(summary_raw)
    source_name = get_source_name(entry)

    link = getattr(entry, "link", "") or ""
    text_all = (title + " " + summary).lower()

    for bad in HARD_EXCLUDE_KEYWORDS:
        if bad.lower() in text_all:
            return -999.0

    low_link = link.lower()
    for hint in HARD_EXCLUDE_URL_HINTS:
        if hint in low_link:
            return -999.0

    # 0) ì—°ì˜ˆ/ê°€ì‹­/ì‚¬ê±´ì‚¬ê³  ë“±ì€ ì•„ì˜ˆ ì œì™¸ (í•˜ë“œ í•„í„°)
    for bad in EXCLUDE_KEYWORDS:
        if bad.lower() in text_all:
            return -999.0  # MIN_SCOREë³´ë‹¤ í›¨ì”¬ ì‘ê²Œ â†’ ë¬´ì¡°ê±´ ë²„ë¦¼
    

    # 1) ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„
    score += source_weight(source_name)

    # 2) ì¸ì‚¬ì´íŠ¸/ë¶„ì„ í‚¤ì›Œë“œ ê°€ì 
    quality_hits = 0
    for kw in QUALITY_KEYWORDS:
        if kw.lower() in text_all:
            quality_hits += 1

    score += min(quality_hits, 2) * 1.0   # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ, ê°€ì¤‘ì¹˜ë„ ë‚®ì¶¤


    # 3) ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì 
    if len(title) < 10:
        score -= 0.5

    # 4) ìš”ì•½ ê¸¸ì´ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
    if len(summary) < 40:
        score -= 0.5

    # 5) ìµœì‹ ì„± (published ê¸°ì¤€)
    published_parsed = getattr(entry, "published_parsed", None)
    if published_parsed:
        published_dt = datetime.datetime(*published_parsed[:6])
        now = datetime.datetime.now()
        delta = now - published_dt

        if delta.days < 1:
            score += 1.3  # 24ì‹œê°„ ì´ë‚´
        elif delta.days < 3:
            score += 1.0  # 3ì¼ ì´ë‚´
        elif delta.days < 7:
            score += 0.7  # 7ì¼ ì´ë‚´
        elif delta.days > 21:
            score -= 1.0  # 3ì£¼ ì´ìƒ ì§€ë‚œ ê¸€ì€ ê°ì 

    return score


# ==========================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°€ê³µ
# ==========================================

def fetch_news_grouped_and_top(sources, top_limit=3):
    """
    - ì£¼ì œë³„(grouped_items)ë¡œ í•„í„°ë§/ìŠ¤ì½”ì–´ ì ìš©ëœ ë‰´ìŠ¤ ëª¨ìŒ
    - ì „ì²´ ê¸°ì‚¬ ì¤‘ TOP N (top_items)

    ê°™ì€ topicì„ ì“°ëŠ” ì—¬ëŸ¬ RSS ì†ŒìŠ¤(KR/EN ë“±)ë¥¼ ëª¨ë‘ í•©ì³ì„œ
    topic ë‹¨ìœ„ë¡œ ì •ë ¬ í›„ ìƒìœ„ limitê°œë§Œ ë‚¨ê¸´ë‹¤.
    """
    print("ğŸ” ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  íë ˆì´íŒ…í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

    grouped_items = {}       # topic -> [item, item, ...]
    seen_titles = set()      # ì „ì²´ ì¤‘ë³µ ì œê±°
    all_items = []           # ì „ì²´ ê¸°ì‚¬ ëª¨ìŒ
    topic_limits = {}        # topicë³„ limit ì„¤ì • (ê°™ì€ topicì˜ ì—¬ëŸ¬ ì†ŒìŠ¤ ì¤‘ ìµœëŒ€ê°’ ì‚¬ìš©)

    for source in sources:
        topic = source["topic"]
        url = source["url"]
        feed_limit = source.get("limit", 3)

        # topicë³„ limit ì¬ì •ì˜: ê°™ì€ topicì´ ì—¬ëŸ¬ ì†ŒìŠ¤ì— ê±¸ì³ ìˆìœ¼ë©´, limitì˜ ìµœëŒ€ê°’ì„ ì‚¬ìš©
        topic_limits[topic] = max(topic_limits.get(topic, 0), feed_limit)

        feed = feedparser.parse(url)

        # ë„ˆë¬´ ë§ì€ ê¸°ì‚¬ ë°©ì§€
        entries = feed.entries[:MAX_ENTRIES_PER_FEED]

        for entry in entries:
            title = getattr(entry, "title", "").strip()
            link = getattr(entry, "link", "").strip()
            summary_raw = getattr(entry, "summary", "") if hasattr(entry, "summary") else ""
            summary_clean = clean_text(summary_raw)
            summary = (summary_clean[:200] + "...") if summary_clean else "ë‚´ìš©ì„ í™•ì¸í•˜ë ¤ë©´ í´ë¦­í•˜ì„¸ìš”."

            if not title:
                continue

            # ì œëª© ê¸°ì¤€ ì „ì—­ ì¤‘ë³µ ì œê±°
            if title in seen_titles:
                continue

            score = score_entry(entry)

            # ìµœì†Œ ì ìˆ˜ ë¯¸ë§Œì´ë©´ ì•„ì˜ˆ ë²„ë¦¼
            if score < MIN_SCORE:
                continue

            seen_titles.add(title)
            published = getattr(entry, "published", None)
            source_name = get_source_name(entry)

            item = {
                "title": title,
                "link": link,
                "summary": summary,
                "published": published,
                "score": score,
                "topic": topic,
                "source": source_name,
            }

            # topicë³„ë¡œ ëˆ„ì 
            if topic not in grouped_items:
                grouped_items[topic] = []
            grouped_items[topic].append(item)

            all_items.append(item)

    # topicë³„ë¡œ ì ìˆ˜ ìˆœ ì •ë ¬ í›„ topicë³„ limitê¹Œì§€ ìë¥´ê¸°
    for topic, items in grouped_items.items():
        items.sort(key=lambda x: x["score"], reverse=True)
        limit_for_topic = topic_limits.get(topic, TOP_LIMIT)
        grouped_items[topic] = items[:limit_for_topic]


    top_items = pick_top_with_mix(all_items, top_limit)

    return grouped_items, top_items



# ==========================================
# MVP JSON ë‚´ë³´ë‚´ê¸° (Lovable/SPA ì†Œë¹„ìš©)
# ==========================================

def map_topic_to_category(topic: str) -> str:
    """í˜„ì¬ RSS topicì„ MVP 3ì¹´í…Œê³ ë¦¬(IT/ê²½ì œ/ê¸€ë¡œë²Œ)ë¡œ ë§¤í•‘."""
    t = (topic or "").lower()
    it_keywords = ["ai", "agi", "ë¡œë´‡", "robot", "ë°˜ë„ì²´", "semiconductor", "ì¸ê³µì§€ëŠ¥"]
    econ_keywords = ["ê²½ì œ", "finance", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì£¼ê°€", "ì¦ì‹œ", "íˆ¬ì", "ì—ë„ˆì§€", "íƒœì–‘ê´‘", "energy"]

    if any(k in t for k in it_keywords):
        return "IT"
    if any(k in t for k in econ_keywords):
        return "ê²½ì œ"
    return "ê¸€ë¡œë²Œ"


def split_summary_to_3lines(summary: str) -> list[str]:
    """ìš”ì•½ ë¬¸ìì—´ì„ ìµœëŒ€ 3ì¤„ ë°°ì—´ë¡œ ë³€í™˜. (MVP UIìš©)"""
    s = (summary or "").strip()
    if not s:
        return []

    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬(ì˜ë¬¸/êµ­ë¬¸ ê³µí†µ) â†’ ìµœëŒ€ 3ê°œ
    parts = [p.strip() for p in re.split(r'(?<=[\.\!\?ã€‚])\s+|(?<=ë‹¤\.)\s+', s) if p.strip()]
    if len(parts) >= 3:
        return parts[:3]

    # ë¬¸ì¥ ë¶„ë¦¬ê°€ ì• ë§¤í•˜ë©´ ê¸¸ì´ë¡œ ê· ë“± ë¶„í• 
    if len(parts) <= 1 and len(s) > 120:
        step = max(40, len(s)//3)
        chunks = [s[i:i+step].strip() for i in range(0, len(s), step)]
        return chunks[:3]

    return parts


def estimate_read_time_seconds(text: str) -> int:
    """í•œêµ­ì–´ í‰ê·  ì½ê¸° ì†ë„ ~500ì/ë¶„ ê°€ì •. 10ì´ˆ ë‹¨ìœ„ ë°˜ì˜¬ë¦¼, 10~40ì´ˆë¡œ í´ë¨í”„."""
    n = len((text or "").strip())
    if n <= 0:
        return 10
    seconds = (n / 500) * 60
    # 10ì´ˆ ë‹¨ìœ„ ë°˜ì˜¬ë¦¼
    rounded = int(round(seconds / 10) * 10)
    return max(10, min(40, rounded))


def _load_existing_digest(path: str = OUTPUT_JSON) -> dict | None:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def _is_valid_digest(digest: dict) -> bool:
    """MVP ì•ˆì „ì¥ì¹˜: 5ê°œ ê³ ì • + í•µì‹¬ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ë§Œ ê²€ì‚¬ (ì—„ê²©í•˜ê²Œ)."""
    if not isinstance(digest, dict):
        return False
    items = digest.get("items")
    if not isinstance(items, list) or len(items) != 5:
        return False

    required_item_keys = {"id", "date", "category", "title", "summary", "sourceName", "sourceUrl", "status", "importance"}
    for it in items:
        if not isinstance(it, dict):
            return False
        if not required_item_keys.issubset(it.keys()):
            return False
        if not it.get("title") or not it.get("sourceUrl"):
            return False
        summary = it.get("summary")
        if not isinstance(summary, list) or len(summary) == 0:
            return False
    return True


def _atomic_write_json(path: str, payload: dict) -> None:
    tmp_path = f"{path}.tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    os.replace(tmp_path, path)


def export_daily_digest_json(top_items: list[dict], output_path: str = OUTPUT_JSON) -> dict:
    """fetch_news_grouped_and_top()ì˜ top_itemsë¥¼ MVP ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜í•´ JSONìœ¼ë¡œ ì €ì¥.
    RSS ì¥ì• /ì˜ˆì™¸ë¡œ 5ê°œë¥¼ ëª» ì±„ìš°ë©´, ê¸°ì¡´ JSONì„ ìœ ì§€í•œë‹¤(ìˆì„ ë•Œ).
    """
    now_kst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))
    date_str = now_kst.strftime("%Y-%m-%d")
    last_updated_at = now_kst.isoformat()

    items_out: list[dict] = []
    for i, item in enumerate(top_items[:5], start=1):
        title = (item.get("title") or "").strip()
        link = (item.get("link") or "").strip()
        summary = (item.get("summary") or "").strip()
        topic = (item.get("topic") or "").strip()
        source_name = (item.get("source") or "").strip()
        published = item.get("published")

        summary_lines = split_summary_to_3lines(summary)
        read_time_sec = estimate_read_time_seconds(" ".join(summary_lines) if summary_lines else summary)

        items_out.append(
            {
                "id": f"{date_str}_{i}",
                "date": date_str,
                "category": map_topic_to_category(topic),
                "title": title,
                "summary": summary_lines if summary_lines else [summary],
                "whyImportant": "",  # MVP: ìˆ˜ë™ ì…ë ¥ ê¶Œì¥ (ì„œë¹„ìŠ¤ ì°¨ë³„í™” í•µì‹¬)
                "sourceName": source_name,
                "sourceUrl": link,
                "publishedAt": published,
                "readTimeSec": read_time_sec,
                "status": "published",
                "importance": 1,
            }
        )

    digest = {
        "date": date_str,
        "selectionCriteria": SELECTION_CRITERIA,
        "editorNote": EDITOR_NOTE,
        "question": QUESTION_OF_THE_DAY,
        "lastUpdatedAt": last_updated_at,
        "items": items_out,
    }

    if not _is_valid_digest(digest):
        existing = _load_existing_digest(output_path)
        if existing and _is_valid_digest(existing):
            print("âš ï¸ ì˜¤ëŠ˜ digest ìƒì„±ì´ ë¶ˆì™„ì „í•˜ì—¬ ê¸°ì¡´ daily_digest.jsonì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            return existing
        raise RuntimeError("digest ìƒì„± ì‹¤íŒ¨: ìœ íš¨í•œ 5ê°œ ë‰´ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ê³  ê¸°ì¡´ íŒŒì¼ë„ ì—†ìŠµë‹ˆë‹¤.")

    _atomic_write_json(output_path, digest)
    return digest


# ==========================================
# HTML ìƒì„±
# ==========================================

def generate_html(grouped_items, top_items):
    print("ğŸ“ HTML ë‰´ìŠ¤ë ˆí„°ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

    html_template = """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="utf-8" />
        <title>{{ title }}</title>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", Arial, sans-serif;
                background-color: #f4f4f4;
                padding: 20px;
            }
            .container {
                max-width: 800px;
                margin: 0 auto;
                background: #ffffff;
                padding: 28px;
                border-radius: 12px;
                box-shadow: 0 5px 20px rgba(0, 0, 0, 0.05);
            }
            h1 {
                color: #1f2933;
                text-align: center;
                border-bottom: 2px solid #e5e7eb;
                padding-bottom: 16px;
                margin-top: 0;
                margin-bottom: 8px;
            }
            .date {
                text-align: center;
                color: #9ca3af;
                font-size: 13px;
                margin-bottom: 20px;
            }
            .intro {
                font-size: 14px;
                color: #4b5563;
                line-height: 1.6;
                margin-bottom: 24px;
            }

            /* TOP 3 ì„¹ì…˜ */
            .top-section {
                margin-bottom: 28px;
                padding: 16px;
                border-radius: 10px;
                background: #f9fafb;
                border: 1px solid #e5e7eb;
            }
            .top-section-title {
                font-size: 16px;
                font-weight: 700;
                color: #111827;
                margin-bottom: 12px;
            }
            .top-list {
                display: grid;
                grid-template-columns: 1fr;
                gap: 12px;
            }
            @media (min-width: 720px) {
                .top-list {
                    grid-template-columns: 1fr 1fr;
                }
            }
            .top-item {
                padding: 12px 14px;
                border-radius: 10px;
                background: #ffffff;
                border: 1px solid #e5e7eb;
            }
            .top-rank {
                font-size: 12px;
                font-weight: 700;
                color: #2563eb;
                margin-bottom: 4px;
            }
            .top-topic {
                font-size: 11px;
                color: #6b7280;
                margin-bottom: 2px;
            }
            .top-source {
                font-size: 11px;
                color: #9ca3af;
                margin-bottom: 4px;
            }
            .top-title {
                font-size: 15px;
                font-weight: 600;
                color: #111827;
                text-decoration: none;
            }
            .top-title:hover {
                text-decoration: underline;
            }
            .top-summary {
                margin-top: 6px;
                font-size: 13px;
                color: #4b5563;
                line-height: 1.5;
            }
            .top-published {
                margin-top: 4px;
                font-size: 11px;
                color: #9ca3af;
            }

            /* ì£¼ì œë³„ ì„¹ì…˜ */
            .topic-section {
                margin-top: 24px;
                margin-bottom: 12px;
                padding-top: 12px;
                border-top: 1px solid #e5e7eb;
            }
            .topic-title {
                font-size: 16px;
                font-weight: 700;
                color: #111827;
                margin-bottom: 10px;
            }
            .news-item {
                margin-bottom: 18px;
            }
            .news-title {
                font-size: 14px;
                font-weight: 600;
                color: #2563eb;
                text-decoration: none;
            }
            .news-title:hover {
                text-decoration: underline;
            }
            .news-summary {
                color: #4b5563;
                font-size: 13px;
                margin-top: 4px;
                line-height: 1.5;
            }
            .published {
                font-size: 11px;
                color: #9ca3af;
                margin-top: 3px;
            }
            .source {
                font-size: 11px;
                color: #9ca3af;
                margin-top: 2px;
            }

            .ad-block {
                background-color: #fff7ed;
                border: 1px solid #fed7aa;
                color: #9a3412;
                padding: 16px;
                text-align: center;
                margin-top: 32px;
                border-radius: 8px;
                font-weight: 600;
                font-size: 14px;
            }
            .ad-link {
                text-decoration: none;
                color: #dc2626;
            }
            .ad-link:hover {
                text-decoration: underline;
            }
            .footer {
                text-align: center;
                font-size: 11px;
                color: #9ca3af;
                margin-top: 24px;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>{{ title }}</h1>
            <div class="date">{{ date }}</div>

            <div class="intro">
                ì˜¤ëŠ˜ ì„¸ê³„ íë¦„ì„ ì½ëŠ” ë° ì¤‘ìš”í•œ
                <strong>AI Â· ë°˜ë„ì²´ Â· ì—ë„ˆì§€ Â· ë°”ì´ì˜¤ Â· ê·œì œ Â· ê¸ˆìœµ</strong> ë‰´ìŠ¤ë¥¼
                í•œ ë²ˆì— ëª¨ì•˜ìŠµë‹ˆë‹¤. ë§¨ ìœ„ì—ëŠ” ê°•í™”ëœ ê¸°ì¤€ìœ¼ë¡œ ì„ ë³„í•œ
                <strong>TOP {{ top_count }} í•µì‹¬ ë‰´ìŠ¤</strong>ê°€, ê·¸ ì•„ë˜ì—ëŠ”
                ì£¼ì œë³„ ì„¹ì…˜ì´ ì´ì–´ì§‘ë‹ˆë‹¤.
            </div>

            {% if top_items %}
            <div class="top-section">
                <div class="top-section-title">ğŸ”¥ ì˜¤ëŠ˜ì˜ í•µì‹¬ TOP {{ top_count }}</div>
                <div class="top-list">
                    {% for item in top_items %}
                    <div class="top-item">
                        <div class="top-rank">TOP {{ loop.index }}</div>
                        <div class="top-topic">{{ item.topic }}</div>
                        {% if item.source %}
                        <div class="top-source">{{ item.source }}</div>
                        {% endif %}
                        <a href="{{ item.link }}" target="_blank" class="top-title">{{ item.title }}</a>
                        {% if item.published %}
                        <div class="top-published">{{ item.published }}</div>
                        {% endif %}
                        <div class="top-summary">{{ item.summary }}</div>
                    </div>
                    {% endfor %}
                </div>
            </div>
            {% endif %}

            {% for topic, items in grouped_items.items() %}
            <div class="topic-section">
                <div class="topic-title">ğŸ“Œ {{ topic }}</div>
                {% for item in items %}
                    <div class="news-item">
                        <a href="{{ item.link }}" class="news-title" target="_blank">ğŸ‘‰ {{ item.title }}</a>
                        {% if item.source %}
                        <div class="source">{{ item.source }}</div>
                        {% endif %}
                        {% if item.published %}
                        <div class="published">{{ item.published }}</div>
                        {% endif %}
                        <p class="news-summary">{{ item.summary }}</p>
                    </div>
                {% endfor %}
            </div>
            {% endfor %}

            <div class="ad-block">
                <a href="{{ ad_link }}" class="ad-link" target="_blank">{{ ad_text }}</a>
            </div>

            <div class="footer">
                Automated by DAILY WORLD v1.0<br />
                ì´ í˜ì´ì§€ëŠ” ê°œì¸ìš© ìë™ ë‰´ìŠ¤ ìš”ì•½ ë´‡ì´ ìƒì„±í–ˆìŠµë‹ˆë‹¤.
            </div>
        </div>
    </body>
    </html>
    """

    template = Template(html_template)
    today = datetime.datetime.now().strftime("%Yë…„ %mì›” %dì¼ (%a)")

    return template.render(
        title=NEWSLETTER_TITLE,
        date=today,
        grouped_items=grouped_items,
        top_items=top_items,
        top_count=len(top_items),
        ad_text=AFFILIATE_AD_TEXT,
        ad_link=AFFILIATE_LINK,
    )


# ==========================================
# ë©”ì¸
# ==========================================



def main():
    try:
        grouped_items, top_items = fetch_news_grouped_and_top(
            RSS_SOURCES, top_limit=TOP_LIMIT
        )

        # 1) MVPìš© JSON ìƒì„± (Lovable/SPAì—ì„œ ë°”ë¡œ ì‚¬ìš©)
        export_daily_digest_json(top_items, OUTPUT_JSON)
        print(f"âœ… ì™„ë£Œ! {OUTPUT_JSON} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 2) (ì„ íƒ) ê¸°ì¡´ HTML ë‰´ìŠ¤ë ˆí„°ë„ ê³„ì† ì“°ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œ
        # html_content = generate_html(grouped_items, top_items)
        # with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
        #     f.write(html_content)
        # print(f"âœ… ì™„ë£Œ! {OUTPUT_FILENAME} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # file_url = "file://" + os.path.realpath(OUTPUT_FILENAME)
        # webbrowser.open(file_url)

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)


if __name__ == "__main__":
    main()
