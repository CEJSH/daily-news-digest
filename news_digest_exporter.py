import feedparser
import datetime
import os
import json
import re
import math
try:
    from dotenv import load_dotenv
except Exception:  # pragma: no cover - optional dependency
    load_dotenv = None
from utils import (
    clean_text, trim_title_noise, get_source_name,
    normalize_title_for_dedupe, jaccard, estimate_read_time_seconds
)
try:
    from ai_enricher import enrich_item_with_ai, get_embedding
except Exception:  # pragma: no cover - optional dependency
    enrich_item_with_ai = None
    get_embedding = None
try:
    from article_fetcher import fetch_article_text
except Exception:  # pragma: no cover - optional dependency
    fetch_article_text = None
from export_manager import export_daily_digest_json
from html_generator import generate_html

if load_dotenv:
    load_dotenv()

# ==========================================
# ì‚¬ìš©ì ì„¤ì • ë° ìƒìˆ˜
# ==========================================

RSS_SOURCES = [
    {"topic": "IT", "url": "https://news.google.com/rss/search?q=AI+ë°˜ë„ì²´+OR+ë°ì´í„°ì„¼í„°+OR+í´ë¼ìš°ë“œ+OR+ë³´ì•ˆ+ì·¨ì•½ì +OR+AI+ê·œì œ+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì›¨ë¹„ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "IT", "url": "https://news.google.com/rss/search?q=AI+chips+OR+data+center+OR+cloud+infrastructure+OR+cybersecurity+vulnerability+OR+AI+regulation+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê²½ì œ", "url": "https://news.google.com/rss/search?q=ê¸ˆë¦¬+OR+í™˜ìœ¨+OR+ë¬¼ê°€+OR+ê³ ìš©+OR+ì‹¤ì +OR+ê²½ê¸°+ì „ë§+OR+ì •ë¶€+ì •ì±…+OR+ì—ë„ˆì§€ì „í™˜+OR+íƒœì–‘ê´‘+OR+ë°”ì´ì˜¤+í—¬ìŠ¤ì¼€ì–´+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "ê²½ì œ", "url": "https://news.google.com/rss/search?q=interest+rate+OR+inflation+OR+fx+OR+jobs+report+OR+earnings+OR+economic+policy+OR+energy+transition+OR+biotech+OR+healthcare+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ì •ì„¸", "url": "https://news.google.com/rss/search?q=ê´€ì„¸+OR+ì œì¬+OR+ë¬´ì—­+OR+ê³µê¸‰ë§+OR+ì™¸êµ+OR+êµ­ì œ+í˜‘ìƒ+-ì‚¬ë§+-ì‚´ì¸+-í­í–‰+-ì—°ì˜ˆ+-ìŠ¤í¬ì¸ +-ë¦¬í¬íŠ¸+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ì •ì„¸", "url": "https://news.google.com/rss/search?q=tariff+OR+sanctions+OR+trade+OR+supply+chain+OR+diplomacy+OR+geopolitics+-opinion+-column+-sports+-celebrity+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ë¹…í…Œí¬", "url": "https://news.google.com/rss/search?q=Apple+OR+Microsoft+OR+Google+OR+OpenAI+OR+NVIDIA+OR+Amazon+OR+Meta+OR+Tesla+OR+TSMC+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ë¹…í…Œí¬", "url": "https://news.google.com/rss/search?q=ì• í”Œ+OR+ë§ˆì´í¬ë¡œì†Œí”„íŠ¸+OR+êµ¬ê¸€+OR+ì˜¤í”ˆAI+OR+ì—”ë¹„ë””ì•„+OR+ì•„ë§ˆì¡´+OR+ë©”íƒ€+OR+TSMC+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 10},
]

QUALITY_KEYWORDS = ["ë¶„ì„", "í•´ì„¤", "ì „ë§", "ì‹¬ì¸µ", "ì§„ë‹¨", "ì „ëµ", "íŒ¨ê¶Œ", "íŒ¨ëŸ¬ë‹¤ì„", "ë³€ê³¡ì ", "êµ¬ì¡°", "ì¬í¸", "ì§€í˜•", "ëª¨ë©˜í…€", "êµ¬ì¡°ì ", "ìƒíƒœê³„", "ì‹œë‚˜ë¦¬ì˜¤", "data", "in-depth", "diagnosis", "strategy", "paradigm", "inflection point", "structure", "reorganization", "ecosystem", "scenario"]
HARD_EXCLUDE_KEYWORDS = ["ë™í–¥", "ë™í–¥ë¦¬í¬íŠ¸", "ë¦¬í¬íŠ¸", "ë¸Œë¦¬í”„", "ë°±ì„œ", "ìë£Œì§‘", "ë³´ê³ ì„œ", "ì—°êµ¬ë³´ê³ ì„œ", "ì„¸ë¯¸ë‚˜", "ì›¨ë¹„ë‚˜", "ì»¨í¼ëŸ°ìŠ¤", "í¬ëŸ¼", "í–‰ì‚¬", "ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜", "ë³´ë„ìë£Œ", "í™ë³´", "í”„ë¡œëª¨ì…˜", "í• ì¸", "ì¶œì‹œê¸°ë…", "ì‚¬ì„¤","ì¹¼ëŸ¼","ê¸°ê³ ","ê¸°ììˆ˜ì²©", "whitepaper", "report", "brief", "webinar", "conference", "forum", "press release", "promotion", "apply now", "opinion","editorial","column","commentary","view","must","should"]
HARD_EXCLUDE_URL_HINTS = ["/report", "/whitepaper", "/webinar", "/seminar", "/conference", "/event", "/download"]
EXCLUDE_KEYWORDS = ["ì—°ì˜ˆ", "ìŠ¤íƒ€", "ê±¸ê·¸ë£¹", "ë³´ì´ê·¸ë£¹", "ì•„ì´ëŒ", "ë°°ìš°", "ê°€ìˆ˜", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì˜í™”", "íŒ¬ë¯¸íŒ…", "ì»´ë°±", "ì•¨ë²”", "ë®¤ì§ë¹„ë””ì˜¤", "ë®¤ë¹„", "í‹°ì €", "í™”ë³´", "ì—´ì• ", "ê²°ë³„", "ì´í˜¼", "ê²°í˜¼", "ì¶œì‚°", "ì•¼êµ¬", "ì¶•êµ¬", "ë†êµ¬", "ë°°êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "Kë¦¬ê·¸", "KBO", "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸", "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸", "ì‚´í•´", "ì‚´ì¸", "í­í–‰", "ì„±í­í–‰", "ê°•ê°„", "ë‚©ì¹˜", "ì‚¬ë§", "ì‹œì‹ ",  "ì§•ì—­", "ë§›ì§‘", "ì¹´í˜", "ë·°ë§›ì§‘", "ì—¬í–‰ê¸°", "ê´€ê´‘ì§€", "ì—°íœ´", "ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "êµí†µí†µì œ", "ê²½ì•…", "ë°œì¹µ", "ì•Œê³ ë³´ë‹ˆ", "ì´ìœ ëŠ”", "ê·¼í™©", "í¬ì°©", "ë§ì‹ ", "ëˆ„ë¦¬ê¾¼", "ê°‘ë¡ ì„ë°•", "ê²°êµ­", "ì •ì²´", "ì¶©ê²©", "í—‰", "ì†Œë¦„", "ì´ê²Œ ì–¼ë§ˆ", "ëŒ€ì°¸ì‚¬", "ëŒ€ë°•", "ì£¼ì˜ë³´", "ë ˆì „ë“œ", "ì›ƒìŒ", "ì›ƒê²¼", "ëˆˆë¬¼", "entertainment", "celebrity", "girl group", "boy group", "idol", "actor", "singer", "variety show", "drama", "movie", "fan meeting", "comeback", "album", "music video", "teaser", "photoshoot", "dating", "breakup", "divorce", "marriage", "childbirth", "baseball", "soccer", "basketball", "volleyball", "golf", "esports", "K League", "KBO", "Premier League", "Champions League", "murder", "killing", "assault", "sexual assault", "rape", "kidnapping", "death", "corpse", "police", "arrest", "detention", "trial", "prison sentence", "lawsuit", "restaurant", "cafe", "tour spot", "travel diary", "tourism", "holiday", "weather", "fine dust", "traffic control", "shock", "scandal", "caught on camera", "backlash", "controversy", "reason why", "latest update", "netizens", "argument", "eventually", "identity", "disaster", "huge", "warning", "legendary", "funny", "laughter", "tearful", "ìì‚¬ë¬´ì†Œ", "ë©´ì‚¬ë¬´ì†Œ", "ë§ˆì„íšŒê´€", "ì²´í—˜ í–‰ì‚¬", "ì§€ì—­ ì†Œì‹", "ì „í†µì‹œì¥", "ì§€ì—­ì£¼ë¯¼", "ë§ˆì„ ì£¼ë¯¼", "ë†ì´Œ ì²´í—˜", "ì–´ì´Œ ì²´í—˜", "ì§€ì—­ ì¶•ì œ", "êµ°ë¯¼", "ê³µëª¨ ì‚¬ì—…"]
SOURCE_TIER_A = {"Reuters", "Bloomberg", "Financial Times", "The Wall Street Journal", "ì—°í•©ë‰´ìŠ¤", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ", "ì„œìš¸ê²½ì œ"}
SOURCE_TIER_B = {"ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì „ìì‹ ë¬¸", "ZDNet Korea", "TechCrunch", "The Verge"}
STOPWORDS = {
    "the", "a", "an", "to", "for", "of", "and", "or", "in", "on", "with",
    "is", "are", "must", "should", "how", "become", "show", "little"
}

IMPACT_SIGNALS_MAP = {
    "policy": ["regulation", "rule", "policy", "bill", "law", "guideline", "government", "ê·œì œ", "ë²•ì•ˆ", "ì •ì±…", "ê°€ì´ë“œë¼ì¸", "ì •ë¶€", "êµ­íšŒ"],
    "budget": ["budget", "fiscal", "appropriation", "incentive", "subsidy", "ì˜ˆì‚°", "ì¬ì •", "ì§€ì›ê¸ˆ", "ì„¸ì œí˜œíƒ"],
    "sanctions": ["sanction", "export control", "entity list", "tariff", "ì œì¬", "ìˆ˜ì¶œí†µì œ", "ë¸”ë™ë¦¬ìŠ¤íŠ¸", "ê´€ì„¸"],
    "capex": ["data center", "datacentre", "capex", "investment", "build", "expansion", "infrastructure", "facility", "ë°ì´í„°ì„¼í„°", "ì¦ì„¤", "íˆ¬ì", "ì„¤ë¹„"],
    "earnings": ["earnings", "guidance", "profit", "loss", "revenue", "í‘ì", "ì ì", "ì‹¤ì ", "ê°€ì´ë˜ìŠ¤", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ"],
    "market-demand": ["registrations", "registration", "deliveries", "delivery", "sales", "demand", "shipments", "ë“±ë¡", "íŒë§¤", "ìˆ˜ìš”"],
    "security": ["breach", "exploit", "ransomware", "cve", "vulnerability", "ì¹¨í•´", "í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì·¨ì•½ì "],
    "infra": ["outage", "downtime", "disruption", "ì¥ì• ", "ì •ì „", "ì„œë¹„ìŠ¤ ì¤‘ë‹¨"]
}

DEDUPE_NOISE_WORDS = {
    "bold", "little", "recovery", "shock", "inside", "first", "new", "top", "best",
    "strategy", "how", "why", "what", "where", "when", "show", "showcase", "unveils",
    "exclusive", "breaking", "update", "latest", "years", "after", "cornerstone", "become",
    "reuters", "bloomberg", "ft", "wsj", "financial", "times", "wall", "street", "journal",
    "ì—°í•©ë‰´ìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´",
    "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "techcrunch", "verge"
}

EMOTIONAL_DROP_KEYWORDS = ["ì°¸ì‚¬", "ì¶©ê²©", "ë¶„ë…¸", "ë…¼ë€", "í­ë¡œ"]
DROP_CATEGORIES = {"ì‚¬íšŒ", "ì‚¬ê±´", "ì—°ì˜ˆ"}

MONTH_TOKENS = {
    "jan", "january", "feb", "february", "mar", "march", "apr", "april", "may", "jun", "june",
    "jul", "july", "aug", "august", "sep", "sept", "september", "oct", "october", "nov", "november",
    "dec", "december"
}

LONG_IMPACT_SIGNALS = {"policy", "budget", "sanctions"}
MEDIA_SUFFIXES = ("ì¼ë³´", "ì‹ ë¬¸", "ë‰´ìŠ¤", "ë°©ì†¡", "ë¯¸ë””ì–´", "tv", "TV")

NEWSLETTER_TITLE = "ğŸš€ DAILY WORLD â€“ AI & Tech ì¼ì¼ ìš”ì•½"
AFFILIATE_AD_TEXT = "ğŸ”¥ ì˜¤ëŠ˜ë§Œ 50% í• ì¸! ìµœê³ ì˜ ìƒì‚°ì„± ë„êµ¬ êµ¬ê²½í•˜ê¸°"
AFFILIATE_LINK = "https://your-affiliate-link.com"
OUTPUT_FILENAME = "daily_world_news.html"
OUTPUT_JSON = "daily_digest.json"
SELECTION_CRITERIA = "â‘  ë‚´ì¼ë„ ì˜í–¥ì´ ë‚¨ëŠ” ì´ìŠˆ â‘¡ ê³¼ë„í•œ ê°ì • ì†Œëª¨ ì œì™¸ â‘¢ ì–´ì œì™€ ì¤‘ë³µë˜ëŠ” ë‰´ìŠ¤ ì œì™¸"
EDITOR_NOTE = "ì´ ë‰´ìŠ¤ëŠ” í´ë¦­ ìˆ˜ê°€ ì•„ë‹ˆë¼ ì˜¤ëŠ˜ ì´í›„ì—ë„ ë‚¨ëŠ” ì •ë³´ë§Œ ê¸°ì¤€ìœ¼ë¡œ í¸ì§‘í–ˆìŠµë‹ˆë‹¤."
QUESTION_OF_THE_DAY = "ì •ë³´ë¥¼ ëœ ë³´ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ë” ë˜‘ë˜‘í•œ ì†Œë¹„ì¼ê¹Œ?"

TOP_LIMIT = 5
MIN_SCORE = 0.0
MAX_ENTRIES_PER_FEED = 100

AI_IMPORTANCE_ENABLED = os.getenv("AI_IMPORTANCE_ENABLED", "1") == "1"
AI_IMPORTANCE_MAX_ITEMS = int(os.getenv("AI_IMPORTANCE_MAX_ITEMS", "30"))
AI_IMPORTANCE_WEIGHT = float(os.getenv("AI_IMPORTANCE_WEIGHT", "1.0"))
AI_QUALITY_ENABLED = os.getenv("AI_QUALITY_ENABLED", "1") == "1"
AI_SEMANTIC_DEDUPE_ENABLED = os.getenv("AI_SEMANTIC_DEDUPE_ENABLED", "1") == "1"
AI_SEMANTIC_DEDUPE_MAX_ITEMS = int(os.getenv("AI_SEMANTIC_DEDUPE_MAX_ITEMS", "50"))
AI_SEMANTIC_DEDUPE_THRESHOLD = float(os.getenv("AI_SEMANTIC_DEDUPE_THRESHOLD", "0.88"))
ARTICLE_FETCH_ENABLED = os.getenv("ARTICLE_FETCH_ENABLED", "1") == "1"
ARTICLE_FETCH_MAX_ITEMS = int(os.getenv("ARTICLE_FETCH_MAX_ITEMS", "20"))
ARTICLE_FETCH_MIN_CHARS = int(os.getenv("ARTICLE_FETCH_MIN_CHARS", "400"))
ARTICLE_FETCH_TIMEOUT_SEC = int(os.getenv("ARTICLE_FETCH_TIMEOUT_SEC", "6"))

# ==========================================
# í•µì‹¬ ë¡œì§ í•¨ìˆ˜
# ==========================================

def get_impact_signals(text: str) -> list[str]:
    signals = []
    text_lower = text.lower()
    for signal, keywords in IMPACT_SIGNALS_MAP.items():
        if any(kw.lower() in text_lower for kw in keywords):
            signals.append(signal)
    return signals

def _tokenize_for_dedupe(text: str) -> list[str]:
    t = clean_text(text or "").lower()
    t = re.sub(r"[^a-z0-9ê°€-í£\s]", " ", t)
    return [x for x in t.split() if x]

def _is_korean_token(token: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", token))

def _is_noise_token(token: str) -> bool:
    if token in STOPWORDS or token in DEDUPE_NOISE_WORDS or token in MONTH_TOKENS:
        return True
    if token.isdigit():
        return True
    if re.search(r"\d", token):
        if token.endswith(("ë…„", "ì›”", "ì¼")) and token[:-1].isdigit():
            return True
    if len(token) == 1:
        return True
    if any(token.endswith(suf) for suf in MEDIA_SUFFIXES):
        return True
    return False

def _valid_token_length(token: str) -> bool:
    if _is_korean_token(token):
        return len(token) >= 2
    return len(token) >= 3

def _strip_source_from_text(text: str, source_name: str) -> str:
    if not text or not source_name:
        return text
    src = re.escape(source_name.strip())
    cleaned = re.sub(rf"(?:\s*[\|\-â€“â€”Â·â€¢:ï½œã…£]\s*)?{src}\s*\.{{0,3}}\s*$", "", text, flags=re.IGNORECASE)
    cleaned = re.sub(rf"\s+{src}\s*\.{{0,3}}\s*$", "", cleaned, flags=re.IGNORECASE)
    return cleaned.strip()

def get_dedupe_key(title: str, summary: str) -> str:
    # 1) í† í°í™” ë° ë…¸ì´ì¦ˆ ì œê±°
    tokens = _tokenize_for_dedupe(f"{title} {summary}")

    # 2) ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë§Œ í•„í„°ë§ (4~8ê°œ ëª©í‘œ)
    seen = set()
    filtered: list[str] = []
    for tok in tokens:
        if tok in seen:
            continue
        if _is_noise_token(tok) or not _valid_token_length(tok):
            continue
        filtered.append(tok)
        seen.add(tok)

    # 3) ë¶€ì¡±í•  ê²½ìš° ì™„í™”ëœ ì¡°ê±´ìœ¼ë¡œ ë³´ì™„
    if len(filtered) < 4:
        for tok in tokens:
            if tok in seen:
                continue
            if tok in STOPWORDS or tok in DEDUPE_NOISE_WORDS or tok in MONTH_TOKENS:
                continue
            if tok.isdigit() or len(tok) < 2:
                continue
            filtered.append(tok)
            seen.add(tok)
            if len(filtered) >= 4:
                break

    # 4) 8ê°œ ì´ˆê³¼ë©´ ê¸¸ì´ ìš°ì„ ìœ¼ë¡œ ìƒìœ„ 8ê°œ ìœ ì§€ (ìˆœì„œëŠ” ì›ë˜ ë“±ì¥ ìˆœì„œ)
    if len(filtered) > 8:
        ranked = sorted(filtered, key=lambda x: (-len(x), filtered.index(x)))
        top = set(ranked[:8])
        filtered = [t for t in filtered if t in top][:8]

    if not filtered:
        fallback = [t for t in tokens if t][:4]
        filtered = fallback if fallback else ["news"]

    return "-".join(filtered).lower()

def map_topic_to_category(topic: str) -> str:
    t = (topic or "").lower()
    if t.startswith("it"): return "IT"
    if "ê²½ì œ" in t: return "ê²½ì œ"
    return "ê¸€ë¡œë²Œ"

def _get_item_category(item: dict) -> str:
    return item.get("aiCategory") or map_topic_to_category(item.get("topic", ""))

def source_weight(source_name: str) -> float:
    s = (source_name or "").strip()
    if any(a in s for a in SOURCE_TIER_A): return 3.0
    if any(b in s for b in SOURCE_TIER_B): return 1.5
    return 0.3

def _compute_age_hours(entry) -> float | None:
    published_parsed = getattr(entry, "published_parsed", None)
    if not published_parsed:
        return None
    published_dt = datetime.datetime(*published_parsed[:6], tzinfo=datetime.timezone.utc)
    now = datetime.datetime.now(datetime.timezone.utc)
    delta = now - published_dt
    return delta.total_seconds() / 3600.0

def _passes_freshness(age_hours: float | None, impact_signals: list[str]) -> bool:
    if age_hours is None:
        return True
    if age_hours > 168:
        return False
    if age_hours > 72 and not any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
        return False
    return True

def _passes_emotional_filter(category: str, text_all: str, impact_signals: list[str]) -> bool:
    if category in DROP_CATEGORIES:
        return False
    if any(k in text_all for k in EMOTIONAL_DROP_KEYWORDS):
        if any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
            return True
        return False
    return True

def score_entry(impact_signals: list[str], read_time_sec: int) -> float:
    score = 0.0
    if any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
        score += 3.0
    if any(s in ["capex", "infra", "security"] for s in impact_signals):
        score += 2.0
    if any(s in ["earnings", "market-demand"] for s in impact_signals):
        score += 1.0
    if read_time_sec <= 20:
        score += 0.5
    return score

def _is_eligible(item: dict) -> bool:
    return not item.get("dropReason")


def pick_top_with_mix(all_items, top_limit=5):
    buckets = {"IT": [], "ê²½ì œ": [], "ê¸€ë¡œë²Œ": []}
    for it in all_items:
        if not _is_eligible(it):
            continue
        buckets[_get_item_category(it)].append(it)

    for cat in buckets:
        buckets[cat].sort(key=lambda x: x["score"], reverse=True)

    target = {"IT": 2, "ê²½ì œ": 2, "ê¸€ë¡œë²Œ": 1}
    picked = []
    for cat, n in target.items():
        picked += buckets[cat][:n]

    if len(picked) < top_limit:
        remain = [
            x for x in sorted(all_items, key=lambda x: x["score"], reverse=True)
            if x not in picked and _is_eligible(x)
        ]
        picked += remain[: top_limit - len(picked)]

    return picked[:top_limit]

def _apply_ai_importance(items: list[dict]) -> None:
    if not AI_IMPORTANCE_ENABLED:
        return
    if enrich_item_with_ai is None:
        return
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        return

    candidates = sorted(items, key=lambda x: x["score"], reverse=True)[:AI_IMPORTANCE_MAX_ITEMS]
    fetch_budget = ARTICLE_FETCH_MAX_ITEMS
    for item in candidates:
        if ARTICLE_FETCH_ENABLED and fetch_article_text and fetch_budget > 0:
            full_text = item.get("fullText") or ""
            if len(full_text) < ARTICLE_FETCH_MIN_CHARS:
                text, resolved_url = fetch_article_text(
                    item.get("link") or "",
                    timeout_sec=ARTICLE_FETCH_TIMEOUT_SEC,
                )
                if text:
                    item["fullText"] = text
                if resolved_url:
                    item["resolvedUrl"] = resolved_url
                fetch_budget -= 1
        ai_result = enrich_item_with_ai(item)
        if not ai_result:
            continue
        item["ai"] = ai_result
        if AI_QUALITY_ENABLED:
            quality_label = ai_result.get("quality_label")
            if quality_label:
                item["aiQuality"] = quality_label
            if quality_label == "low_quality":
                reason = ai_result.get("quality_reason") or "ai_low_quality"
                item["dropReason"] = f"ai_low_quality:{reason}"
                item["aiQualityTags"] = ai_result.get("quality_tags") or []
                continue
        ai_category = ai_result.get("category_label")
        if ai_category:
            item["aiCategory"] = ai_category
        impact_signals_ai = ai_result.get("impact_signals") or []
        if impact_signals_ai:
            merged = sorted(set((item.get("impactSignals") or []) + impact_signals_ai))
            item["impactSignals"] = merged
            read_time_sec = item.get("readTimeSec")
            if not read_time_sec:
                summary_raw = item.get("summaryRaw") or item.get("summary") or ""
                read_time_sec = estimate_read_time_seconds(summary_raw)
                item["readTimeSec"] = read_time_sec
            item["score"] = score_entry(merged, read_time_sec)
        importance = ai_result.get("importance_score")
        if not importance:
            continue
        item["aiImportance"] = importance
        item["score"] = max(0.0, item["score"] + (importance - 3) * AI_IMPORTANCE_WEIGHT)


def _cosine_similarity(a: list[float], b: list[float]) -> float:
    if not a or not b:
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(y * y for y in b))
    if norm_a == 0.0 or norm_b == 0.0:
        return 0.0
    return dot / (norm_a * norm_b)


def _dedupe_text(item: dict) -> str:
    title = item.get("title") or ""
    summary_raw = item.get("summaryRaw") or item.get("summary") or ""
    return clean_text(f"{title} {summary_raw}")


def _apply_semantic_dedupe(items: list[dict]) -> None:
    if not AI_SEMANTIC_DEDUPE_ENABLED:
        return
    if get_embedding is None:
        return
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        return

    candidates = sorted(items, key=lambda x: x["score"], reverse=True)[:AI_SEMANTIC_DEDUPE_MAX_ITEMS]
    kept: list[dict] = []
    for item in candidates:
        if not _is_eligible(item):
            continue
        text = _dedupe_text(item)
        if not text:
            continue
        embedding = get_embedding(text)
        if not embedding:
            continue
        item["embedding"] = embedding
        is_dup = False
        for ref in kept:
            ref_emb = ref.get("embedding")
            if not ref_emb:
                continue
            sim = _cosine_similarity(embedding, ref_emb)
            if sim >= AI_SEMANTIC_DEDUPE_THRESHOLD:
                item["dropReason"] = f"semantic_duplicate:{ref.get('title','')[:60]}"
                item["matchedTo"] = ref.get("id") or ref.get("dedupeKey") or ref.get("title")
                is_dup = True
                break
        if not is_dup:
            kept.append(item)

def _load_yesterday_dedupe_map(path: str) -> dict[str, str]:
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            digest = json.load(f)
    except Exception:
        return {}

    now_kst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))
    yesterday = (now_kst - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
    if digest.get("date") != yesterday:
        return {}

    items = digest.get("items", [])
    dedupe_map: dict[str, str] = {}
    for it in items:
        if not isinstance(it, dict):
            continue
        if it.get("status") not in {"published", "kept"}:
            continue
        key = it.get("dedupeKey")
        item_id = it.get("id")
        if key and item_id:
            dedupe_map[key] = item_id
        if item_id:
            title = it.get("title") or ""
            summary = it.get("summary") or []
            summary_text = " ".join(summary) if isinstance(summary, list) else str(summary)
            alt_key = get_dedupe_key(title, summary_text)
            if alt_key:
                dedupe_map[alt_key] = item_id
    return dedupe_map

def fetch_news_grouped_and_top(sources, top_limit=3):
    print("ğŸ” ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  íë ˆì´íŒ…í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    grouped_items, seen_titles, all_items, topic_limits = {}, set(), [], {}
    seen_title_tokens: list[tuple[set[str], dict]] = []
    seen_items_by_dedupe_key = {}
    yesterday_dedupe_map = _load_yesterday_dedupe_map(OUTPUT_JSON)

    for source in sources:
        topic, url, feed_limit = source["topic"], source["url"], source.get("limit", 3)
        topic_limits[topic] = max(topic_limits.get(topic, 0), feed_limit)
        feed = feedparser.parse(url)
        
        for entry in feed.entries[:MAX_ENTRIES_PER_FEED]:
            title = getattr(entry, "title", "").strip()
            summary_raw = getattr(entry, "summary", "") if hasattr(entry, "summary") else ""
            source_name = get_source_name(entry)
            summary_clean = clean_text(summary_raw)
            summary_clean = _strip_source_from_text(summary_clean, source_name)
            title_clean = trim_title_noise(clean_text(title), source_name)
            summary = (summary_clean[:200] + "...") if summary_clean else "ë‚´ìš©ì„ í™•ì¸í•˜ë ¤ë©´ í´ë¦­í•˜ì„¸ìš”."
            full_text = ""
            content_list = getattr(entry, "content", None)
            if isinstance(content_list, list) and content_list:
                parts = []
                for c in content_list:
                    value = ""
                    if isinstance(c, dict):
                        value = c.get("value", "") or ""
                    else:
                        value = getattr(c, "value", "") or ""
                    if value:
                        parts.append(value)
                if parts:
                    full_text = clean_text(" ".join(parts))
            if not full_text:
                full_text = summary_clean

            tokens = normalize_title_for_dedupe(title_clean, STOPWORDS)
            text_all = (title_clean + " " + summary_clean).lower()
            impact_signals = get_impact_signals(text_all)
            dedupe_key = get_dedupe_key(title_clean, summary_clean)
            matched_to = yesterday_dedupe_map.get(dedupe_key)

            kept_item = next((p_item for p_tok, p_item in seen_title_tokens if jaccard(tokens, p_tok) >= 0.6), None)
            if not kept_item:
                kept_item = seen_items_by_dedupe_key.get(dedupe_key)

            if kept_item:
                kept_item.setdefault("mergedSources", []).append({"title": title_clean, "link": entry.link, "source": get_source_name(entry)})
                continue

            if title in seen_titles: continue
            link = getattr(entry, "link", "") or ""
            category = map_topic_to_category(topic)
            age_hours = _compute_age_hours(entry)

            if any(bad.lower() in text_all for bad in HARD_EXCLUDE_KEYWORDS): continue
            if any(hint in link.lower() for hint in HARD_EXCLUDE_URL_HINTS): continue
            if any(bad.lower() in text_all for bad in EXCLUDE_KEYWORDS if bad not in EMOTIONAL_DROP_KEYWORDS): continue

            if matched_to:
                continue

            if not impact_signals:
                continue

            if not _passes_freshness(age_hours, impact_signals):
                continue

            if not _passes_emotional_filter(category, text_all, impact_signals):
                continue

            read_time_sec = estimate_read_time_seconds(summary_clean)
            score = score_entry(impact_signals, read_time_sec)
            if score < MIN_SCORE:
                continue

            seen_titles.add(title)
            item = {
                "title": title_clean, "link": entry.link, "summary": summary,
                "summaryRaw": summary_clean,
                "fullText": full_text,
                "published": getattr(entry, "published", None), "score": score,
                "topic": topic, "source": source_name,
                "impactSignals": impact_signals, "dedupeKey": dedupe_key, "matchedTo": matched_to,
                "readTimeSec": read_time_sec
            }
            seen_title_tokens.append((tokens, item))
            seen_items_by_dedupe_key[dedupe_key] = item
            grouped_items.setdefault(topic, []).append(item)
            all_items.append(item)

    _apply_ai_importance(all_items)
    _apply_semantic_dedupe(all_items)

    for topic, items in grouped_items.items():
        filtered = [x for x in items if _is_eligible(x)]
        filtered.sort(key=lambda x: x["score"], reverse=True)
        grouped_items[topic] = filtered[:topic_limits.get(topic, TOP_LIMIT)]

    return grouped_items, pick_top_with_mix(all_items, top_limit)

def main():
    try:
        grouped_items, top_items = fetch_news_grouped_and_top(RSS_SOURCES, top_limit=TOP_LIMIT)
        
        config = {
            "newsletter_title": NEWSLETTER_TITLE,
            "ad_text": AFFILIATE_AD_TEXT,
            "ad_link": AFFILIATE_LINK,
            "selection_criteria": SELECTION_CRITERIA,
            "editor_note": EDITOR_NOTE,
            "question": QUESTION_OF_THE_DAY
        }

        export_daily_digest_json(top_items, OUTPUT_JSON, config)
        print(f"âœ… ì™„ë£Œ! {OUTPUT_JSON} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)

if __name__ == "__main__":
    main()
