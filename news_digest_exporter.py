import feedparser
import datetime
import os
import json
import re
from utils import (
    clean_text, trim_title_noise, get_source_name,
    normalize_title_for_dedupe, jaccard, estimate_read_time_seconds
)
from export_manager import export_daily_digest_json
from html_generator import generate_html

# ==========================================
# ì‚¬ìš©ì ì„¤ì • ë° ìƒìˆ˜
# ==========================================

RSS_SOURCES = [
    {"topic": "IT", "url": "https://news.google.com/rss/search?q=AI+ë°˜ë„ì²´+OR+ë°ì´í„°ì„¼í„°+OR+í´ë¼ìš°ë“œ+OR+ë³´ì•ˆ+ì·¨ì•½ì +OR+AI+ê·œì œ+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì›¨ë¹„ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "IT", "url": "https://news.google.com/rss/search?q=AI+chips+OR+data+center+OR+cloud+infrastructure+OR+cybersecurity+vulnerability+OR+AI+regulation+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê²½ì œ", "url": "https://news.google.com/rss/search?q=ê¸ˆë¦¬+OR+í™˜ìœ¨+OR+ë¬¼ê°€+OR+ê³ ìš©+OR+ì‹¤ì +OR+ê²½ê¸°+ì „ë§+OR+ì •ë¶€+ì •ì±…+OR+ì—ë„ˆì§€ì „í™˜+OR+íƒœì–‘ê´‘+OR+ë°”ì´ì˜¤+í—¬ìŠ¤ì¼€ì–´+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "ê²½ì œ", "url": "https://news.google.com/rss/search?q=interest+rate+OR+inflation+OR+fx+OR+jobs+report+OR+earnings+OR+economic+policy+OR+energy+transition+OR+biotech+OR+healthcare+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ì •ì„¸", "url": "https://news.google.com/rss/search?q=ê´€ì„¸+OR+ì œì¬+OR+ë¬´ì—­+OR+ê³µê¸‰ë§+OR+ì™¸êµ+OR+êµ­ì œ+í˜‘ìƒ+-ì‚¬ë§+-ì‚´ì¸+-í­í–‰+-ì—°ì˜ˆ+-ìŠ¤í¬ì¸ +-ë¦¬í¬íŠ¸+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ì •ì„¸", "url": "https://news.google.com/rss/search?q=tariff+OR+sanctions+OR+trade+OR+supply+chain+OR+diplomacy+OR+geopolitics+-opinion+-column+-sports+-celebrity+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ë¹…í…Œí¬", "url": "https://news.google.com/rss/search?q=Apple+OR+Microsoft+OR+Google+OR+OpenAI+OR+NVIDIA+OR+Amazon+OR+Meta+OR+Tesla+OR+TSMC+-opinion+-column+-webinar+-whitepaper&hl=en&gl=US&ceid=US:en", "limit": 15},
    {"topic": "ê¸€ë¡œë²Œ_ë¹…í…Œí¬", "url": "https://news.google.com/rss/search?q=ì• í”Œ+OR+ë§ˆì´í¬ë¡œì†Œí”„íŠ¸+OR+êµ¬ê¸€+OR+ì˜¤í”ˆAI+OR+ì—”ë¹„ë””ì•„+OR+ì•„ë§ˆì¡´+OR+ë©”íƒ€+OR+TSMC+-ë¦¬í¬íŠ¸+-ì„¸ë¯¸ë‚˜+-ì¹¼ëŸ¼&hl=ko&gl=KR&ceid=KR:ko", "limit": 10},
]

QUALITY_KEYWORDS = ["ë¶„ì„", "í•´ì„¤", "ì „ë§", "ì‹¬ì¸µ", "ì§„ë‹¨", "ì „ëµ", "íŒ¨ê¶Œ", "íŒ¨ëŸ¬ë‹¤ì„", "ë³€ê³¡ì ", "êµ¬ì¡°", "ì¬í¸", "ì§€í˜•", "ëª¨ë©˜í…€", "êµ¬ì¡°ì ", "ìƒíƒœê³„", "ì‹œë‚˜ë¦¬ì˜¤", "data", "in-depth", "diagnosis", "strategy", "paradigm", "inflection point", "structure", "reorganization", "ecosystem", "scenario"]
HARD_EXCLUDE_KEYWORDS = ["ë™í–¥", "ë™í–¥ë¦¬í¬íŠ¸", "ë¦¬í¬íŠ¸", "ë¸Œë¦¬í”„", "ë°±ì„œ", "ìë£Œì§‘", "ë³´ê³ ì„œ", "ì—°êµ¬ë³´ê³ ì„œ", "ì„¸ë¯¸ë‚˜", "ì›¨ë¹„ë‚˜", "ì»¨í¼ëŸ°ìŠ¤", "í¬ëŸ¼", "í–‰ì‚¬", "ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜", "ë³´ë„ìë£Œ", "í™ë³´", "í”„ë¡œëª¨ì…˜", "í• ì¸", "ì¶œì‹œê¸°ë…", "ì‚¬ì„¤","ì¹¼ëŸ¼","ê¸°ê³ ","ê¸°ììˆ˜ì²©", "whitepaper", "report", "brief", "webinar", "conference", "forum", "press release", "promotion", "apply now", "opinion","editorial","column","commentary","view","must","should"]
HARD_EXCLUDE_URL_HINTS = ["/report", "/whitepaper", "/webinar", "/seminar", "/conference", "/event", "/download"]
EXCLUDE_KEYWORDS = ["ì—°ì˜ˆ", "ìŠ¤íƒ€", "ê±¸ê·¸ë£¹", "ë³´ì´ê·¸ë£¹", "ì•„ì´ëŒ", "ë°°ìš°", "ê°€ìˆ˜", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì˜í™”", "íŒ¬ë¯¸íŒ…", "ì»´ë°±", "ì•¨ë²”", "ë®¤ì§ë¹„ë””ì˜¤", "ë®¤ë¹„", "í‹°ì €", "í™”ë³´", "ì—´ì• ", "ê²°ë³„", "ì´í˜¼", "ê²°í˜¼", "ì¶œì‚°", "ì•¼êµ¬", "ì¶•êµ¬", "ë†êµ¬", "ë°°êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "Kë¦¬ê·¸", "KBO", "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸", "ì±”í”¼ì–¸ìŠ¤ë¦¬ê·¸", "ì‚´í•´", "ì‚´ì¸", "í­í–‰", "ì„±í­í–‰", "ê°•ê°„", "ë‚©ì¹˜", "ì‚¬ë§", "ì‹œì‹ ",  "ì§•ì—­", "ë§›ì§‘", "ì¹´í˜", "ë·°ë§›ì§‘", "ì—¬í–‰ê¸°", "ê´€ê´‘ì§€", "ì—°íœ´", "ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "êµí†µí†µì œ", "ê²½ì•…", "ë°œì¹µ", "ì•Œê³ ë³´ë‹ˆ", "ì´ìœ ëŠ”", "ê·¼í™©", "í¬ì°©", "ë§ì‹ ", "ëˆ„ë¦¬ê¾¼", "ê°‘ë¡ ì„ë°•", "ê²°êµ­", "ì •ì²´", "ì¶©ê²©", "í—‰", "ì†Œë¦„", "ì´ê²Œ ì–¼ë§ˆ", "ëŒ€ì°¸ì‚¬", "ëŒ€ë°•", "ì£¼ì˜ë³´", "ë ˆì „ë“œ", "ì›ƒìŒ", "ì›ƒê²¼", "ëˆˆë¬¼", "entertainment", "celebrity", "girl group", "boy group", "idol", "actor", "singer", "variety show", "drama", "movie", "fan meeting", "comeback", "album", "music video", "teaser", "photoshoot", "dating", "breakup", "divorce", "marriage", "childbirth", "baseball", "soccer", "basketball", "volleyball", "golf", "esports", "K League", "KBO", "Premier League", "Champions League", "murder", "killing", "assault", "sexual assault", "rape", "kidnapping", "death", "corpse", "police", "arrest", "detention", "trial", "prison sentence", "lawsuit", "restaurant", "cafe", "tour spot", "travel diary", "tourism", "holiday", "weather", "fine dust", "traffic control", "shock", "scandal", "caught on camera", "backlash", "controversy", "reason why", "latest update", "netizens", "argument", "eventually", "identity", "disaster", "huge", "warning", "legendary", "funny", "laughter", "tearful", "ìì‚¬ë¬´ì†Œ", "ë©´ì‚¬ë¬´ì†Œ", "ë§ˆì„íšŒê´€", "ì²´í—˜ í–‰ì‚¬", "ì§€ì—­ ì†Œì‹", "ì „í†µì‹œì¥", "ì§€ì—­ì£¼ë¯¼", "ë§ˆì„ ì£¼ë¯¼", "ë†ì´Œ ì²´í—˜", "ì–´ì´Œ ì²´í—˜", "ì§€ì—­ ì¶•ì œ", "êµ°ë¯¼", "ê³µëª¨ ì‚¬ì—…"]
SOURCE_TIER_A = {"Reuters", "Bloomberg", "Financial Times", "The Wall Street Journal", "ì—°í•©ë‰´ìŠ¤", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ", "ì„œìš¸ê²½ì œ"}
SOURCE_TIER_B = {"ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì „ìì‹ ë¬¸", "ZDNet Korea", "TechCrunch", "The Verge"}
STOPWORDS = {
    "the", "a", "an", "to", "for", "of", "and", "or", "in", "on", "with",
    "is", "are", "must", "should", "how", "become", "show", "little"
}

IMPACT_SIGNALS_MAP = {
    "policy": ["regulation", "rule", "policy", "bill", "law", "guideline", "government", "ê·œì œ", "ë²•ì•ˆ", "ì •ì±…", "ê°€ì´ë“œë¼ì¸", "ì •ë¶€", "êµ­íšŒ"],
    "budget": ["budget", "fiscal", "appropriation", "incentive", "subsidy", "ì˜ˆì‚°", "ì¬ì •", "ì§€ì›ê¸ˆ", "ì„¸ì œí˜œíƒ"],
    "sanctions": ["sanction", "export control", "entity list", "tariff", "ì œì¬", "ìˆ˜ì¶œí†µì œ", "ë¸”ë™ë¦¬ìŠ¤íŠ¸", "ê´€ì„¸"],
    "capex": ["data center", "datacentre", "capex", "investment", "build", "expansion", "infrastructure", "facility", "ë°ì´í„°ì„¼í„°", "ì¦ì„¤", "íˆ¬ì", "ì„¤ë¹„"],
    "earnings": ["earnings", "guidance", "profit", "loss", "revenue", "í‘ì", "ì ì", "ì‹¤ì ", "ê°€ì´ë˜ìŠ¤", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ"],
    "market-demand": ["registrations", "registration", "deliveries", "delivery", "sales", "demand", "shipments", "ë“±ë¡", "íŒë§¤", "ìˆ˜ìš”"],
    "security": ["breach", "exploit", "ransomware", "cve", "vulnerability", "ì¹¨í•´", "í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì·¨ì•½ì "],
    "infra": ["outage", "downtime", "disruption", "ì¥ì• ", "ì •ì „", "ì„œë¹„ìŠ¤ ì¤‘ë‹¨"]
}

DEDUPE_NOISE_WORDS = {
    "bold", "little", "recovery", "shock", "inside", "first", "new", "top", "best",
    "strategy", "how", "why", "what", "where", "when", "show", "showcase", "unveils",
    "exclusive", "breaking", "update", "latest", "years", "after", "cornerstone", "become",
    "reuters", "bloomberg", "ft", "wsj", "financial", "times", "wall", "street", "journal",
    "ì—°í•©ë‰´ìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´",
    "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "techcrunch", "verge"
}

EMOTIONAL_DROP_KEYWORDS = ["ì°¸ì‚¬", "ì¶©ê²©", "ë¶„ë…¸", "ë…¼ë€", "í­ë¡œ"]
DROP_CATEGORIES = {"ì‚¬íšŒ", "ì‚¬ê±´", "ì—°ì˜ˆ"}

MONTH_TOKENS = {
    "jan", "january", "feb", "february", "mar", "march", "apr", "april", "may", "jun", "june",
    "jul", "july", "aug", "august", "sep", "sept", "september", "oct", "october", "nov", "november",
    "dec", "december"
}

LONG_IMPACT_SIGNALS = {"policy", "budget", "sanctions"}
MEDIA_SUFFIXES = ("ì¼ë³´", "ì‹ ë¬¸", "ë‰´ìŠ¤", "ë°©ì†¡", "ë¯¸ë””ì–´", "tv", "TV")

NEWSLETTER_TITLE = "ğŸš€ DAILY WORLD â€“ AI & Tech ì¼ì¼ ìš”ì•½"
AFFILIATE_AD_TEXT = "ğŸ”¥ ì˜¤ëŠ˜ë§Œ 50% í• ì¸! ìµœê³ ì˜ ìƒì‚°ì„± ë„êµ¬ êµ¬ê²½í•˜ê¸°"
AFFILIATE_LINK = "https://your-affiliate-link.com"
OUTPUT_FILENAME = "daily_world_news.html"
OUTPUT_JSON = "daily_digest.json"
SELECTION_CRITERIA = "â‘  ë‚´ì¼ë„ ì˜í–¥ì´ ë‚¨ëŠ” ì´ìŠˆ â‘¡ ê³¼ë„í•œ ê°ì • ì†Œëª¨ ì œì™¸ â‘¢ ì–´ì œì™€ ì¤‘ë³µë˜ëŠ” ë‰´ìŠ¤ ì œì™¸"
EDITOR_NOTE = "ì´ ë‰´ìŠ¤ëŠ” í´ë¦­ ìˆ˜ê°€ ì•„ë‹ˆë¼ ì˜¤ëŠ˜ ì´í›„ì—ë„ ë‚¨ëŠ” ì •ë³´ë§Œ ê¸°ì¤€ìœ¼ë¡œ í¸ì§‘í–ˆìŠµë‹ˆë‹¤."
QUESTION_OF_THE_DAY = "ì •ë³´ë¥¼ ëœ ë³´ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ë” ë˜‘ë˜‘í•œ ì†Œë¹„ì¼ê¹Œ?"

TOP_LIMIT = 5
MIN_SCORE = 0.0
MAX_ENTRIES_PER_FEED = 100

# ==========================================
# í•µì‹¬ ë¡œì§ í•¨ìˆ˜
# ==========================================

def get_impact_signals(text: str) -> list[str]:
    signals = []
    text_lower = text.lower()
    for signal, keywords in IMPACT_SIGNALS_MAP.items():
        if any(kw.lower() in text_lower for kw in keywords):
            signals.append(signal)
    return signals

def _tokenize_for_dedupe(text: str) -> list[str]:
    t = clean_text(text or "").lower()
    t = re.sub(r"[^a-z0-9ê°€-í£\s]", " ", t)
    return [x for x in t.split() if x]

def _is_korean_token(token: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", token))

def _is_noise_token(token: str) -> bool:
    if token in STOPWORDS or token in DEDUPE_NOISE_WORDS or token in MONTH_TOKENS:
        return True
    if token.isdigit():
        return True
    if re.search(r"\d", token):
        if token.endswith(("ë…„", "ì›”", "ì¼")) and token[:-1].isdigit():
            return True
    if len(token) == 1:
        return True
    if any(token.endswith(suf) for suf in MEDIA_SUFFIXES):
        return True
    return False

def _valid_token_length(token: str) -> bool:
    if _is_korean_token(token):
        return len(token) >= 2
    return len(token) >= 3

def _strip_source_from_text(text: str, source_name: str) -> str:
    if not text or not source_name:
        return text
    src = re.escape(source_name.strip())
    cleaned = re.sub(rf"(?:\s*[\|\-â€“â€”Â·â€¢:ï½œã…£]\s*)?{src}\s*\.{{0,3}}\s*$", "", text, flags=re.IGNORECASE)
    cleaned = re.sub(rf"\s+{src}\s*\.{{0,3}}\s*$", "", cleaned, flags=re.IGNORECASE)
    return cleaned.strip()

def get_dedupe_key(title: str, summary: str) -> str:
    # 1) í† í°í™” ë° ë…¸ì´ì¦ˆ ì œê±°
    tokens = _tokenize_for_dedupe(f"{title} {summary}")

    # 2) ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë§Œ í•„í„°ë§ (4~8ê°œ ëª©í‘œ)
    seen = set()
    filtered: list[str] = []
    for tok in tokens:
        if tok in seen:
            continue
        if _is_noise_token(tok) or not _valid_token_length(tok):
            continue
        filtered.append(tok)
        seen.add(tok)

    # 3) ë¶€ì¡±í•  ê²½ìš° ì™„í™”ëœ ì¡°ê±´ìœ¼ë¡œ ë³´ì™„
    if len(filtered) < 4:
        for tok in tokens:
            if tok in seen:
                continue
            if tok in STOPWORDS or tok in DEDUPE_NOISE_WORDS or tok in MONTH_TOKENS:
                continue
            if tok.isdigit() or len(tok) < 2:
                continue
            filtered.append(tok)
            seen.add(tok)
            if len(filtered) >= 4:
                break

    # 4) 8ê°œ ì´ˆê³¼ë©´ ê¸¸ì´ ìš°ì„ ìœ¼ë¡œ ìƒìœ„ 8ê°œ ìœ ì§€ (ìˆœì„œëŠ” ì›ë˜ ë“±ì¥ ìˆœì„œ)
    if len(filtered) > 8:
        ranked = sorted(filtered, key=lambda x: (-len(x), filtered.index(x)))
        top = set(ranked[:8])
        filtered = [t for t in filtered if t in top][:8]

    if not filtered:
        fallback = [t for t in tokens if t][:4]
        filtered = fallback if fallback else ["news"]

    return "-".join(filtered).lower()

def map_topic_to_category(topic: str) -> str:
    t = (topic or "").lower()
    if t.startswith("it"): return "IT"
    if "ê²½ì œ" in t: return "ê²½ì œ"
    return "ê¸€ë¡œë²Œ"

def source_weight(source_name: str) -> float:
    s = (source_name or "").strip()
    if any(a in s for a in SOURCE_TIER_A): return 3.0
    if any(b in s for b in SOURCE_TIER_B): return 1.5
    return 0.3

def _compute_age_hours(entry) -> float | None:
    published_parsed = getattr(entry, "published_parsed", None)
    if not published_parsed:
        return None
    published_dt = datetime.datetime(*published_parsed[:6], tzinfo=datetime.timezone.utc)
    now = datetime.datetime.now(datetime.timezone.utc)
    delta = now - published_dt
    return delta.total_seconds() / 3600.0

def _passes_freshness(age_hours: float | None, impact_signals: list[str]) -> bool:
    if age_hours is None:
        return True
    if age_hours > 168:
        return False
    if age_hours > 72 and not any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
        return False
    return True

def _passes_emotional_filter(category: str, text_all: str, impact_signals: list[str]) -> bool:
    if category in DROP_CATEGORIES:
        return False
    if any(k in text_all for k in EMOTIONAL_DROP_KEYWORDS):
        if any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
            return True
        return False
    return True

def score_entry(impact_signals: list[str], read_time_sec: int) -> float:
    score = 0.0
    if any(s in LONG_IMPACT_SIGNALS for s in impact_signals):
        score += 3.0
    if any(s in ["capex", "infra", "security"] for s in impact_signals):
        score += 2.0
    if any(s in ["earnings", "market-demand"] for s in impact_signals):
        score += 1.0
    if read_time_sec <= 20:
        score += 0.5
    return score

def pick_top_with_mix(all_items, top_limit=5):
    buckets = {"IT": [], "ê²½ì œ": [], "ê¸€ë¡œë²Œ": []}
    for it in all_items:
        buckets[map_topic_to_category(it.get("topic", ""))].append(it)

    for cat in buckets:
        buckets[cat].sort(key=lambda x: x["score"], reverse=True)

    target = {"IT": 2, "ê²½ì œ": 2, "ê¸€ë¡œë²Œ": 1}
    picked = []
    for cat, n in target.items():
        picked += buckets[cat][:n]

    if len(picked) < top_limit:
        remain = [x for x in sorted(all_items, key=lambda x: x["score"], reverse=True) if x not in picked]
        picked += remain[: top_limit - len(picked)]

    return picked[:top_limit]

def _load_yesterday_dedupe_map(path: str) -> dict[str, str]:
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            digest = json.load(f)
    except Exception:
        return {}

    now_kst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))
    yesterday = (now_kst - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
    if digest.get("date") != yesterday:
        return {}

    items = digest.get("items", [])
    dedupe_map: dict[str, str] = {}
    for it in items:
        if not isinstance(it, dict):
            continue
        if it.get("status") not in {"published", "kept"}:
            continue
        key = it.get("dedupeKey")
        item_id = it.get("id")
        if key and item_id:
            dedupe_map[key] = item_id
    return dedupe_map

def fetch_news_grouped_and_top(sources, top_limit=3):
    print("ğŸ” ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  íë ˆì´íŒ…í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    grouped_items, seen_titles, all_items, topic_limits = {}, set(), [], {}
    seen_title_tokens: list[tuple[set[str], dict]] = []
    seen_items_by_dedupe_key = {}
    yesterday_dedupe_map = _load_yesterday_dedupe_map(OUTPUT_JSON)

    for source in sources:
        topic, url, feed_limit = source["topic"], source["url"], source.get("limit", 3)
        topic_limits[topic] = max(topic_limits.get(topic, 0), feed_limit)
        feed = feedparser.parse(url)
        
        for entry in feed.entries[:MAX_ENTRIES_PER_FEED]:
            title = getattr(entry, "title", "").strip()
            summary_raw = getattr(entry, "summary", "") if hasattr(entry, "summary") else ""
            source_name = get_source_name(entry)
            summary_clean = clean_text(summary_raw)
            summary_clean = _strip_source_from_text(summary_clean, source_name)
            title_clean = trim_title_noise(clean_text(title), source_name)
            summary = (summary_clean[:200] + "...") if summary_clean else "ë‚´ìš©ì„ í™•ì¸í•˜ë ¤ë©´ í´ë¦­í•˜ì„¸ìš”."

            tokens = normalize_title_for_dedupe(title_clean, STOPWORDS)
            text_all = (title_clean + " " + summary_clean).lower()
            impact_signals = get_impact_signals(text_all)
            dedupe_key = get_dedupe_key(title_clean, summary_clean)
            matched_to = yesterday_dedupe_map.get(dedupe_key)

            kept_item = next((p_item for p_tok, p_item in seen_title_tokens if jaccard(tokens, p_tok) >= 0.6), None)
            if not kept_item:
                kept_item = seen_items_by_dedupe_key.get(dedupe_key)

            if kept_item:
                kept_item.setdefault("mergedSources", []).append({"title": title_clean, "link": entry.link, "source": get_source_name(entry)})
                continue

            if title in seen_titles: continue
            link = getattr(entry, "link", "") or ""
            category = map_topic_to_category(topic)
            age_hours = _compute_age_hours(entry)

            if any(bad.lower() in text_all for bad in HARD_EXCLUDE_KEYWORDS): continue
            if any(hint in link.lower() for hint in HARD_EXCLUDE_URL_HINTS): continue
            if any(bad.lower() in text_all for bad in EXCLUDE_KEYWORDS if bad not in EMOTIONAL_DROP_KEYWORDS): continue

            if matched_to:
                continue

            if not impact_signals:
                continue

            if not _passes_freshness(age_hours, impact_signals):
                continue

            if not _passes_emotional_filter(category, text_all, impact_signals):
                continue

            read_time_sec = estimate_read_time_seconds(summary_clean)
            score = score_entry(impact_signals, read_time_sec)
            if score < MIN_SCORE:
                continue

            seen_titles.add(title)
            item = {
                "title": title_clean, "link": entry.link, "summary": summary,
                "published": getattr(entry, "published", None), "score": score,
                "topic": topic, "source": source_name,
                "impactSignals": impact_signals, "dedupeKey": dedupe_key, "matchedTo": matched_to,
                "readTimeSec": read_time_sec
            }
            seen_title_tokens.append((tokens, item))
            seen_items_by_dedupe_key[dedupe_key] = item
            grouped_items.setdefault(topic, []).append(item)
            all_items.append(item)

    for topic, items in grouped_items.items():
        items.sort(key=lambda x: x["score"], reverse=True)
        grouped_items[topic] = items[:topic_limits.get(topic, TOP_LIMIT)]

    return grouped_items, pick_top_with_mix(all_items, top_limit)

def main():
    try:
        grouped_items, top_items = fetch_news_grouped_and_top(RSS_SOURCES, top_limit=TOP_LIMIT)
        
        config = {
            "newsletter_title": NEWSLETTER_TITLE,
            "ad_text": AFFILIATE_AD_TEXT,
            "ad_link": AFFILIATE_LINK,
            "selection_criteria": SELECTION_CRITERIA,
            "editor_note": EDITOR_NOTE,
            "question": QUESTION_OF_THE_DAY
        }

        export_daily_digest_json(top_items, OUTPUT_JSON, config)
        print(f"âœ… ì™„ë£Œ! {OUTPUT_JSON} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)

if __name__ == "__main__":
    main()
